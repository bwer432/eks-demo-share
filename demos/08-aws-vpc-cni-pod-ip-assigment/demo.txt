=====================================================================
GIVEN:
  - A developer desktop with docker & git installed (AWS Cloud9)
  - An EKS cluster named 'cluster-eksctl' has been created via 04-create-advanced-cluster-eksctl-existing-vpc
WHEN:
  - I deploy a simple workload
THEN:
  - I will ssh onto a node
SO THAT:
  - I can see how the AWS VCP CNI sets up Pod IP assignment

=====================================================================
(0) Requires

    04-create-advanced-cluster-eksctl-existing-vpc

(0) PreReqs

    (-) In the C9 instance IDE ... Set Required Key variables for Bash commands to refer to:

        export C9_REGION=$(curl --silent http://169.254.169.254/latest/dynamic/instance-identity/document |  grep region | awk -F '"' '{print$4}')
        echo $C9_REGION

        export C9_AWS_ACCT=$(curl -s http://169.254.169.254/latest/dynamic/instance-identity/document | grep accountId | awk -F '"' '{print$4}')
        echo $C9_AWS_ACCT

    (-) !!! In the C9 instance IDE ... Edit prefs to not use automated AWS credentials and pass your AWS Keys to the CLI otherwise eksctl & kubectl commands WILL FAIL !!!

        ---> Cloud9/Preferences
          ---> AWS Settings
            ---> Disable AWS managed temporary credentials radial button

        aws configure <--- Add your AWS account user Keys

        export AWS_ACCESS_KEY_ID=$(cat ~/.aws/credentials | grep aws_access_key_id | awk '{print$3}')
        export AWS_SECRET_ACCESS_KEY=$(cat ~/.aws/credentials | grep aws_secret_access_key | awk '{print$3}')
        aws sts get-caller-identity

(1) Setup eksctl & kubectl.

    (-) Install kubectl & set context:

        curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
        curl -LO "https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256"
        echo "$(<kubectl.sha256) kubectl" | sha256sum --check
        sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl

        eksctl utils write-kubeconfig --cluster cluster-eksctl --region $C9_REGION --authenticator-role-arn arn:aws:iam::${C9_AWS_ACCT}:role/cluster-eksctl-creator-role
        kubectl config view --minify
        kubectl get all -A

    (-) Install eksctl:

        curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
        sudo mv /tmp/eksctl /usr/local/bin
        eksctl version

(2) Deploy a simple workload.

    (-) Deploy 1 x replica of game-2048:

        cd ~/environment/mglab-share-eks/demos/08-aws-vpc-cni-pod-ip-assigment/
        kubectl apply -f ./artifacts/08-DEMO-ingress-app.yaml

        kubectl get pod -o wide -n game-2048

    (-) Get Pod IP to compare on host route tables

        export POD_IP=$(kubectl get pod -o wide -n game-2048 | grep deployment-2048 | awk '{print$6}')
        echo $POD_IP

(3) SSH into the node to explore networking.

    (-) Get host private ip that game-2048 is running on:

        export EC2_NODE=$(kubectl get pod -o wide -n game-2048 | grep deployment-2048 | awk '{print$7}' | tr '-' '\.' | awk -F '.' '{print$2"."$3"."$4"."$5}')
        echo $EC2_NODE

    (-) Allow SSH from the Cloud9 desktop to the SG for the nodegroups

        export C9_IP=$(curl -s http://169.254.169.254/latest/dynamic/instance-identity/document | jq .privateIp | tr -d '"')
        echo $C9_IP

        export EC2_SGS=$(aws ec2 describe-network-interfaces --filters Name=addresses.private-ip-address,Values=${EC2_NODE} --query NetworkInterfaces[].Groups[].GroupId)
        echo $EC2_SGS

        echo $EC2_SGS | jq -c '.[]' | while read i; do
          echo "Setting Rule to allow SSH in for nodes in $i from C9 instance"
          SG=$(echo $i | tr -d '"')
          aws ec2 authorize-security-group-ingress --group-id $SG --protocol tcp --port 22 --cidr ${C9_IP}/32
        done

    (-) SSH into instance, all remaining cmds in this section occur on the node:

        chmod 500 ../04-create-advanced-cluster-eksctl-existing-vpc/cluster_eksctl_key.pem
        ssh -i ../04-create-advanced-cluster-eksctl-existing-vpc/cluster_eksctl_key2.pem ec2-user@${EC2_NODE}

        (-) Get the container ids of a workload you want to inspect.  You should see also pause container for every pod running on the node:

            docker ps -a | grep 2048

            export CTR_ID=$(docker ps -a | grep docker-2048 | awk '{print$1}')
            export CTR_ID_PAUSE=$(docker ps -a | grep 2048 | grep pause | awk '{print$1}')

            echo $CTR_ID
            echo $CTR_ID_PAUSE

        (-) Now get the pod veth(A) ID mapped using the app container:

            docker exec -it $CTR_ID ash -c 'cat /sys/class/net/eth0/iflink'

            export CTR_VETH_POD=$(docker exec -it $CTR_ID ash -c 'cat /sys/class/net/eth0/iflink')

            echo $CTR_VETH_POD

        (-) Now get the host namespace veth(B) 'linked' to the pod network namespace using that veth(A) pod ID from the container:

            grep -l [[ INSERT VAL OF $CTR_VETH_POD HERE ]] /sys/class/net/*/ifindex

            export CTR_VETH_NODE=$(grep -l 8 /sys/class/net/*/ifindex | awk -F '/' '{print$5}')
            echo $CTR_VETH_NODE

        (-) You can see this link via the ip link list command, notice it has its own network names space id for the 2 linked veths:

            ip link list | grep $CTR_VETH_NODE

        (-) Now lets look at how those veths route in the host to get/in out:
        (-) Get IP of Pod using pause container ID:

            CMD="curl -s http://localhost:61679/v1/enis | jq '.ENIs[].IPv4Addresses[] | select(.IPAMKey.containerID|startswith(\"$CTR_ID_PAUSE\")) | .Address'"
            export CTR_IP=$(eval $CMD | tr -d '"')
            echo $CTR_IP

        (-) Get inbound/outbound rules from host for that pod:

            ip rule list | grep $CTR_IP

        (-) Get the Inbound route for traffic coming into the pod, we can see the route table send all traffic in for the pod to our veth :)

            ip route show table main | grep $CTR_IP
            echo $CTR_VETH_NODE



(CLEANUP)

    (-) Cleanup Demo Script(s)
